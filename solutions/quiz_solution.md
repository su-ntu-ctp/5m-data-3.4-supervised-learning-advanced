# Quiz Solutions

### Q1: What is the primary purpose of regularization in machine learning?

- A. To increase model complexity
- **B. To prevent overfitting**
- C. To improve data preprocessing
- D. To reduce training time

### Q2: Which of the following is NOT a type of regularization discussed in the notebook?

- A. L1 Regularization (Lasso)
- B. L2 Regularization (Ridge)
- C. Elastic Net
- **D. Principal Component Analysis (PCA)**

### Q3: What is the effect of L1 regularization on feature coefficients?

- A. It ensures all coefficients are non-zero
- **B. It can zero out some feature coefficients, performing feature selection**
- C. It makes coefficients larger
- D. It has no effect on feature coefficients

### Q4: In the context of regularization, what does the parameter Î± (alpha) control?

- A. The type of regularization (L1 or L2)
- **B. The strength of the penalty**
- C. The number of features in the model
- D. The size of the training dataset

### Q5: What is a key benefit of using cross-validation for hyperparameter tuning?

- A. It guarantees the best possible model performance
- **B. It provides a more reliable estimate of out-of-sample performance**
- C. It reduces the need for a separate test set
- D. It speeds up the training process

### Q6: What is the main difference between Grid Search and Random Search for hyperparameter tuning?

- A. Grid Search is faster than Random Search
- **B. Grid Search evaluates all possible combinations, while Random Search samples a subset**
- C. Random Search is more suitable for small hyperparameter spaces
- D. There is no difference between them

### Q7: What is Gini impurity used for in the context of decision trees?

- A. To measure the height of the tree
- B. To determine which branch to prune
- **C. To determine the best feature to split on**
- D. To measure the depth of the tree

### Q8: What is the maximum value of entropy for binary classification?

- A. 0
- B. 0.5
- **C. 1**
- D. 2

### Q9: What is the key difference between Bagging and Boosting?

- A. Bagging is sequential, while Boosting is parallel
- B. Bagging aims to reduce bias, while Boosting aims to reduce variance
- **C. Bagging trains models independently, while Boosting trains models sequentially, focusing on misclassified examples**
- D. There is no difference between Bagging and Boosting

### Q10: Which of the following is a key advantage of LightGBM?

- A. It always outperforms other gradient boosting frameworks
- B. It is designed for small datasets
- **C. It offers faster training and higher efficiency**
- D. It requires minimal hyperparameter tuning
